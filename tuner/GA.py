import os
import sys
import copy
import json
import logging
import argparse
from tqdm import tqdm

import torch
import time
import numpy as np
import pandas as pd

import utils

sys.path.append('../')

from models.steps import (sinlge_fitness_function, twice_fitness_function, prepareForGA)
from models.dnn import RedisSingleDNN, RedisTwiceDNN

parser = argparse.ArgumentParser()
parser.add_argument('--target', type = str, default = '1', help='Target Workload')
parser.add_argument('--persistence', type = str, choices = ["RDB","AOF"], default = 'RDB', help='Choose Persistant Methods')
parser.add_argument('--topk', type = int, default=4,)
parser.add_argument('--path',type= str)
parser.add_argument('--num', type = str)
parser.add_argument('--n_pool',type = int, default = 64)
parser.add_argument('--n_generation', type=int, default=10000,)
parser.add_argument("--model_mode", type = str, default = 'single', help = "model mode")

args = parser.parse_args()

if not os.path.exists('save_knobs'):
    assert "Do this file after running main.py"

def mse_loss(target, predict):
    return np.array([(target[:,i]-predict[:,i])**2 for i in range(2)]).sum(axis=0)

def ATR_loss(default, predict, weight):
    return sum([((-1**i)*weight[i]*(predict[:,i]-default[:,i]))/default[:,i] for i in range(2)])


print("======================MAKE GA LOGGER====================")
logger, log_dir = utils.get_logger(os.path.join('./GA_logs'))

def generate_config(top_k_knobs, final_solution_pool):
    """ Generate config file according to the top_k_knobs and final_solution_pool which are generated by GA

    Args:
        top_k_knobs (list[str]): the name of top k knobs
        final_solution_pool (DataFrame): the final value of top k knobs
    
    Return:
        config file: contain the top k config instead of original config
    """
    rdb_initial_config = json.load(open('../data/rdb_knobs.json', 'r'))

    top_k_knobs_value = list(final_solution_pool.iloc[0, 0:4])
    top_k_knobs_config = {value: top_k_knobs_value[i]
                        for i, value in enumerate(top_k_knobs)}
    memory_dict, active_dict = dict(), dict()
    for key, value in rdb_initial_config.items():
        if 'memory' in key:
            memory_dict[key] = value
        elif 'defrag' in key:
            active_dict[key] = value
        if value == 'yes' or value == 'no':
            if key in top_k_knobs_config.keys():
                if top_k_knobs_config[key] == 0:
                    top_k_knobs_config[key] = 'no'
                else:
                    top_k_knobs_config[key] = 'yes'
    policy_dict = ["volatile-lru", "allkeys-lru", "volatile-lfu", "allkeys-lfu", 
                "volatile-random", "allkeys-random", "volatile-ttl", "noeviction"]

    # the rule of activedefrag related config
    if 'activedefrag' in top_k_knobs_config.keys():
        if top_k_knobs_config['activedefrag'] == 'no':
            for key, value in active_dict.items():
                del rdb_initial_config[key]
                rdb_initial_config["#"+key] = value
        else:
            for key in active_dict.keys():
                if key in top_k_knobs_config.keys():
                    rdb_initial_config[key] = top_k_knobs_config[key]
                    del top_k_knobs_config[key]
    else:
        for key in active_dict.keys():
            if key in top_k_knobs_config.keys():
                rdb_initial_config[key] = top_k_knobs_config[key]
                del top_k_knobs_config[key]

    # the rule of maxmemory related config
    for key in memory_dict.keys():
        if key in top_k_knobs_config.keys():
            if key == 'maxmemory':
                if top_k_knobs_config[key] == 1 or top_k_knobs_config[key] == 2 or top_k_knobs_config[key] == 3:
                    top_k_knobs_config[key] = int(top_k_knobs_config[key])
                rdb_initial_config[key] = str(top_k_knobs_config[key]) + 'gb'
                del top_k_knobs_config[key]
            elif key == 'maxmemory-policy':
                policy_id = top_k_knobs_config[key]
                rdb_initial_config[key] = policy_dict[policy_id]
                del top_k_knobs_config[key]
            elif key == 'maxmemory-samples':
                rdb_initial_config[key] = top_k_knobs_config[key]
                del top_k_knobs_config[key]
        if 'maxmemory' not in top_k_knobs_config.keys():
            rdb_initial_config['maxmemory'] = '32mb' if args.target in range(1, 10) else '1gb'

    # deal with the rest knobs
    for key, value in top_k_knobs_config.items():
        rdb_initial_config[key] = value
    
    with open("../data/init_config.conf", "r") as read_file:
        lines = read_file.readlines() + ['\n']
        with open("./GA_config/1.conf", "a+") as write_file:
            write_file.writelines(lines)
            for key, value in rdb_initial_config.items():
                if 'sec' in key or 'changes' in key:
                    line += f' {value}'
                    i += 1
                    if i < 2: 
                        continue
                    else:
                        write_file.write(line + '\n')
                else:
                    line = f'{key} ' + f'{value}'
                    write_file.write(line + '\n')
                i, line = 0, 'save'

def server_connection():
    import paramiko

    client = paramiko.SSHClient()
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    client.connect('34.64.156.201', username='jieun', password='1423')

    sftp = client.open_sftp()
    sftp.put('./GA_config/1.conf', './redis-sample-generation/1.conf')
    _, ssh_stdout, _ = client.exec_command('python ./redis-sample-generation/connection.py rdb ./redis-sample-generation/1.conf')
    exit_status = ssh_stdout.channel.recv_exit_status()
    if exit_status == 0:
        sftp.get('/home/jieun/result_rdb_external_default_18.csv', './GA_config/result_rdb_external_default_18.csv')
        sftp.get('/home/jieun/result_rdb_internal_default_18.csv', './GA_config/result_rdb_internal_default_18.csv')
    sftp.close()
    client.exec_command('rm ./redis-sample-generation/1.conf')
    client.exec_command('rm /home/jieun/result_rdb_external_default_18.csv')
    client.exec_command('rm /home/jieun/result_rdb_internal_default_18.csv')

    client.close()

def main():
    top_k_knobs = np.load(os.path.join('./save_knobs',args.path,f"knobs_{args.topk}.npy"))
    if args.model_mode == 'single':
        model = RedisSingleDNN(args.topk+5,2)
        model.load_state_dict(torch.load(os.path.join('./model_save',args.path,'model_{}.pt'.format(args.num))))
        fitness_function = sinlge_fitness_function
    if args.model_mode == 'twice':
        model = RedisTwiceDNN(args.topk+5,2)
        model.load_state_dict(torch.load(os.path.join('./model_save',args.path,'model_{}.pt'.format(args.num))))
        fitness_function = twice_fitness_function
    pruned_configs, external_data, default, scaler_X, scaler_y = prepareForGA(args,top_k_knobs)
    temp_configs = pd.concat([pruned_configs,external_data],axis=1)
    temp_configs = temp_configs.sort_values(["Totals_Ops/sec","Totals_p99_Latency"], ascending=[False,True])
    target = temp_configs[["Totals_Ops/sec","Totals_p99_Latency"]].values[0]
    configs = temp_configs.drop(columns=["Totals_Ops/sec","Totals_p99_Latency"])
    n_configs = top_k_knobs.shape[0]
    n_pool_half = args.n_pool//2
    mutation = int(n_configs*0.7)
    current_solution_pool = configs[:args.n_pool].values
    target = np.repeat([default], args.n_pool, axis = 0)
    for i in tqdm(range(args.n_generation)):
        scaled_pool = scaler_X.transform(current_solution_pool)
        predicts = fitness_function(scaled_pool, args, model)
        fitness = scaler_y.inverse_transform(predicts)
        idx_fitness = ATR_loss(target, fitness,(0.5,0.5))
        sorted_idx_fitness = np.argsort(idx_fitness)[n_pool_half:]
        best_solution_pool = current_solution_pool[sorted_idx_fitness,:]
        if i % 1000 == 999:
            print(f"[{i+1:3d}/{args.n_generation:3d}] best fitness: {max(idx_fitness)}")
        pivot = np.random.choice(np.arange(1,n_configs))
        new_solution_pool = np.zeros_like(best_solution_pool)
        for j in range(n_pool_half):
            new_solution_pool[j][:pivot] = best_solution_pool[j][:pivot]
            new_solution_pool[j][pivot:n_configs] = best_solution_pool[n_pool_half-1-j][pivot:n_configs]
            new_solution_pool[j][n_configs:] = current_solution_pool[0][n_configs:]
            import utils, random
            random_knobs = utils.make_random_option(top_k_knobs)
            knobs = list(random_knobs.values())
            random_knob_index = list(range(n_configs))
            random.shuffle(random_knob_index)
            random_knob_index = random_knob_index[:mutation]
            for k in range(len(random_knob_index)):
                new_solution_pool[j][random_knob_index[k]] = knobs[random_knob_index[k]]
        current_solution_pool = np.vstack([best_solution_pool, new_solution_pool])
    final_solution_pool = pd.DataFrame(best_solution_pool)
    print(top_k_knobs)
    generate_config(top_k_knobs, final_solution_pool)
    server_connection()

if __name__ == '__main__':
    try:
        main()
    except:
        logger.exception("ERROR")
    finally:
        logger.handlers.clear()
        logging.shutdown()
